{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from sklearn.preprocessing import LabelEncoder                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_bank = pd.read_csv(\"./grammer_data.csv\", header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대분류</th>\n",
       "      <th>소분류</th>\n",
       "      <th>난이도</th>\n",
       "      <th>문장</th>\n",
       "      <th>해석</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>문장 형식</td>\n",
       "      <td>주어+완전자동사 (S+V)</td>\n",
       "      <td>하</td>\n",
       "      <td>You win.</td>\n",
       "      <td>네가 이겼어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>문장 형식</td>\n",
       "      <td>주어+완전자동사 (S+V)</td>\n",
       "      <td>하</td>\n",
       "      <td>Going up?</td>\n",
       "      <td>올라가나요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>문장 형식</td>\n",
       "      <td>주어+완전자동사 (S+V)</td>\n",
       "      <td>하</td>\n",
       "      <td>Let's see.</td>\n",
       "      <td>어디 보자.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>문장 형식</td>\n",
       "      <td>주어+완전자동사 (S+V)</td>\n",
       "      <td>하</td>\n",
       "      <td>Let's try.</td>\n",
       "      <td>한 번 해봅시다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>문장 형식</td>\n",
       "      <td>주어+완전자동사 (S+V)</td>\n",
       "      <td>하</td>\n",
       "      <td>Speak out.</td>\n",
       "      <td>거리낌 없이 얘기 해.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     대분류             소분류 난이도          문장            해석\n",
       "0  문장 형식  주어+완전자동사 (S+V)   하    You win.      네가 이겼어. \n",
       "1  문장 형식  주어+완전자동사 (S+V)   하   Going up?        올라가나요?\n",
       "2  문장 형식  주어+완전자동사 (S+V)   하  Let's see.       어디 보자. \n",
       "3  문장 형식  주어+완전자동사 (S+V)   하  Let's try.    한 번 해봅시다. \n",
       "4  문장 형식  주어+완전자동사 (S+V)   하  Speak out.  거리낌 없이 얘기 해."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammer_bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset={}\n",
    "for category in grammer_bank['catetory'].unique():\n",
    "    dataset[category] = grammer_bank[grammer_bank['catetory']==category][['catetory','detail','sentence']]\n",
    "    encoder = LabelEncoder()\n",
    "    dataset[category]['detail']=encoder.fit_transform(dataset[category]['detail'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['가능 여부 표현하기', '가능 여부 표현하기', '가능 여부 표현하기', ..., '인물묘사하기', '인물묘사하기',\n",
       "       '인물묘사하기'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(encoder.inverse_transform(dataset[category]['detail']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['문장 형식', '문장의 종류', '문장 구성 요소', '명사 (nouns)', '대명사 (referents)', '동사 (verbs)', '시제 (tense)', '조동사 (modals)', '형용사 (adjectives)', '부사 (adverbs)', '전치사 (prepositions)', '접속사 (conjunctions)', '한정사 (determiners)', '부정 (negation)', '도치 강조 생략 삽입', '분사 (participle)', '동명사 (gerunds)', '부정사 (infinitives)', '비교 (comparison)', '관계사 (relatives)', '태 (voice)', '가정법 (subjunctives)', '문장 전환 (transformation)', '빈출 관용 구문', '숙어 (Idioms)', '의사소통 (communicatives)'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import glob, os\n",
    "\n",
    "os.makedirs('./dataset', exist_ok=True)\n",
    "english_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "df_dict={}\n",
    "for idx, key in enumerate(dataset.keys()):\n",
    "    labels = [i for i in dataset[key]['detail']]\n",
    "    texts = [i for i in dataset[key]['문장']]\n",
    "    df_dict[key] = pd.DataFrame(zip(texts, labels), columns = ['prompt','completion']) #[:300]\n",
    "    en_key= key if english_pattern.findall(key)==[] else english_pattern.findall(key)[0]\n",
    "    df_json = df_dict[key].to_json(f\"dataset/{en_key}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['부사 (adverbs)']['completion'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (0.26.5)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from openai) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from openai) (4.50.2)\n",
      "Requirement already satisfied: aiohttp in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests>=2.20->openai) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests>=2.20->openai) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests>=2.20->openai) (1.25.10)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (20.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aiohttp->openai) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 1545 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 1 duplicated prompt-completion sets. These are rows: [334]\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 1 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
      "/Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages/openai/validators.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"prompt\"] += suffix\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "/Users/aohus/.pyenv/versions/3.8.3/lib/python3.8/site-packages/openai/validators.py:421: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[\"completion\"] = x[\"completion\"].apply(\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `dataset/sentence_type_prepared_train.jsonl` and `dataset/sentence_type_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"dataset/sentence_type_prepared_train.jsonl\" -v \"dataset/sentence_type_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 22\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
      "Once your model starts training, it'll approximately take 39.39 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f dataset/sentence_type.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "l=[]\n",
    "with jsonlines.open(\"dataset/sentence_type_prepared_train.jsonl\") as f:\n",
    "    for line in f.iter():\n",
    "    \tl.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|████████████████████| 83.9k/83.9k [00:00<00:00, 39.0Mit/s]\n",
      "Uploaded file from dataset/adverbs_prepared_train.jsonl: file-KpvgZ7CpjOdRYTPQt2FyCOwq\n",
      "Upload progress: 100%|████████████████████| 21.3k/21.3k [00:00<00:00, 19.1Mit/s]\n",
      "Uploaded file from dataset/adverbs_prepared_valid.jsonl: file-D8jG8Z6m3YeEb4WnleNdV4HD\n",
      "Created fine-tune: ft-ngWnYhR0kLLcLGzoWuhntAXF\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-02-14 21:45:57] Created fine-tune: ft-ngWnYhR0kLLcLGzoWuhntAXF\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-ngWnYhR0kLLcLGzoWuhntAXF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"dataset/sentence_type_prepared_train.jsonl\" -v \"dataset/sentence_type_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 22  -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-14 21:45:57] Created fine-tune: ft-ngWnYhR0kLLcLGzoWuhntAXF\n",
      "[2023-02-14 21:50:56] Fine-tune failed. Errors:\n",
      "The number of classes in file-D8jG8Z6m3YeEb4WnleNdV4HD does not match the number of classes specified in the hyperparameters.\n",
      "\n",
      "Job failed. Please contact support@openai.com if you need assistance.\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-ngWnYhR0kLLcLGzoWuhntAXF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./result'):\n",
    "    os.makedirs('./result')\n",
    "    print(\"make result folder done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mError:\u001b[0m No results file available for fine-tune ft-ngWnYhR0kLLcLGzoWuhntAXF\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.results -i ft-ngWnYhR0kLLcLGzoWuhntAXF > result/adverbs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-14 19:23:14] Created fine-tune: ft-7Eye2bO8HEV1Ycu224FvVMjD\n",
      "[2023-02-14 19:31:00] Fine-tune costs $0.11\n",
      "[2023-02-14 19:31:00] Fine-tune enqueued. Queue number: 2\n",
      "[2023-02-14 19:31:40] Fine-tune is in the queue. Queue number: 1\n",
      "[2023-02-14 19:34:43] Fine-tune is in the queue. Queue number: 0\n",
      "[2023-02-14 19:40:10] Fine-tune started\n",
      "[2023-02-14 19:40:10] If `compute_classification_metrics` is `True`, each of the classes must start with a different token. You can view your class tokenizations at https://platform.openai.com/tokenizer?view=bpe.. Fine-tune failed. For help, please contact OpenAI and include your fine-tune ID: ft-7Eye2bO8HEV1Ycu224FvVMjD\n",
      "\n",
      "Job failed. Please contact support@openai.com if you need assistance.\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-7Eye2bO8HEV1Ycu224FvVMjD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('result/result.csv')\n",
    "results[results['classification/accuracy'].notnull()].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['classification/accuracy'].notnull()]['classification/accuracy'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('sport2_prepared_valid.jsonl', lines=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = 'ada:ft-openai-2021-07-30-12-26-20'\n",
    "res = openai.Completion.create(model=ft_model, prompt=test['prompt'][0] + '\\n\\n###\\n\\n', max_tokens=1, temperature=0)\n",
    "res['choices'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a2c010aadc3fd780168d8c2be8e18d5e63a83389aea6b88b2f2175232a4f4ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
